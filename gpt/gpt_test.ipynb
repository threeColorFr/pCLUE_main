{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "text_generation_zh = pipeline(Tasks.text_generation, model='outputs/Lgpt/output/')\n",
    "result_zh = text_generation_zh(\"这是一个测试\")\n",
    "print(result_zh['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 12:36:22,225 - modelscope - INFO - Model revision not specified, use the latest revision: v1.0.0\n",
      "2022-11-27 12:36:22,396 - modelscope - INFO - File config.json already in cache, skip downloading!\n",
      "2022-11-27 12:36:22,397 - modelscope - INFO - File configuration.json already in cache, skip downloading!\n",
      "2022-11-27 12:36:22,398 - modelscope - INFO - File gpt.png already in cache, skip downloading!\n",
      "2022-11-27 12:36:22,400 - modelscope - INFO - File pytorch_model.bin already in cache, skip downloading!\n",
      "2022-11-27 12:36:22,401 - modelscope - INFO - File README.md already in cache, skip downloading!\n",
      "2022-11-27 12:36:22,401 - modelscope - INFO - File vocab.txt already in cache, skip downloading!\n",
      "2022-11-27 12:36:22,423 - modelscope - INFO - The key of sentence1: None, The key of sentence2: None, The key of label: label\n",
      "2022-11-27 12:36:22,424 - modelscope - WARNING - [Important] first_sequence attribute is not set, this will cause an error if your input is a dict.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 17059,  6393,   202,   244,  8213, 16031,  6225,  6646,     3,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])} torch.Size([1, 500])\n"
     ]
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "from modelscope.preprocessors import TextGenerationPreprocessor\n",
    "\n",
    "model_id = 'damo/nlp_gpt3_text-generation_chinese-large'\n",
    "model_dir = snapshot_download(model_id)\n",
    "sentence = '这是一个测试文本'\n",
    "\n",
    "preprocessor = TextGenerationPreprocessor(model_dir, sequence_length=500)\n",
    "result = preprocessor(sentence)\n",
    "print(result, result['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用tokenizer，效果一样的\n",
    "- 这里gpt会有特殊token cls和sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 17059,  6393,   202,   244,  8213, 16031,  6225,  6646,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# model_dir 为模型配置文件路径，需包含 vocab.txt 文件\n",
    "tokenizer = BertTokenizer.from_pretrained('nlp_gpt3_text-generation_chinese-large')\n",
    "output = tokenizer('这是一个测试文本', return_tensors='pt')\n",
    "print(output)\n",
    "# 将tokenizer的输出传入模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向传播与生成\n",
    "1.   传的是个字典，然后传labels就会计算loss，不然就是None。返回值为(loss, logits)\n",
    "- labels可以传-100，交叉熵会忽略，这是nn.CrossEntropyLoss自带的\n",
    "2.  训练过程可以看情况把labels调整忽略不计算loss\n",
    "3. huggingface官方默认是把输入当作labels（去掉第一个字），logits会去掉最后一个字符(here)[https://huggingface.co/transformers/v4.6.0/_modules/transformers/models/gpt2/modeling_gpt2.html#GPT2LMHeadModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:59:45,470 - modelscope - INFO - initialize model from nlp_gpt3_text-generation_chinese-large\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 25600])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelscope.models.nlp import PalmForTextGeneration\n",
    "from modelscope.models.nlp import GPT3ForTextGeneration\n",
    "from modelscope.models import Model\n",
    "model = GPT3ForTextGeneration.from_pretrained('nlp_gpt3_text-generation_chinese-large')\n",
    "model(result)['logits'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate只支持一句的，本质是只取了第一句，然后去掉[sep]特殊符号，调用huggingface的generate。\n",
    "- 返回值是id，要用tokenizer.decode解码\n",
    "- 这里默认是do_sample的，所以每次生成不一样，具体看源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2, 17059,  6393,   202,   244,  8213, 16031,  6225,  6646, 21124,\n",
       "         6393,   202,   244,  2546, 12681, 10574,  8213, 16031, 21124,  6393,\n",
       "          202,   244,  1721,   431,  3779,  6225,  6646, 17061, 15126,  1386])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "result['max_length'] = 30\n",
    "seq = model.generate(result)\n",
    "seq['sequences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这 是 一 个 测 试 文 本 ， 是 一 个 在 线 的 测 试 ， 是 一 个 可 以 对 文 本 进 行 加'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(seq['sequences'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/weixiang/.miniconda3/envs/modelscope/lib/python3.7/site-packages/modelscope/models/__init__.py'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modelscope.models\n",
    "modelscope.models.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT3Config {\n",
       "  \"apply_query_key_layer_scaling\": true,\n",
       "  \"apply_residual_connection_post_layernorm\": false,\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_softmax_in_fp32\": false,\n",
       "  \"attention_type\": \"self\",\n",
       "  \"bf16\": false,\n",
       "  \"bias_dropout_fusion\": true,\n",
       "  \"bias_gelu_fusion\": true,\n",
       "  \"eod_id\": 7,\n",
       "  \"ffn_hidden_size\": 4096,\n",
       "  \"fp16\": false,\n",
       "  \"fp32_residual_connection\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout\": 0.1,\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"init_method_std\": 0.02,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"kv_channels\": 64,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"layernorm_epsilon\": 1e-12,\n",
       "  \"masked_softmax_fusion\": true,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt3\",\n",
       "  \"no_persist_layer_norm\": false,\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"sequence_parallel\": false,\n",
       "  \"tokens_to_generate\": 100,\n",
       "  \"top_k\": 0,\n",
       "  \"top_p\": 0.9,\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 25600\n",
       "}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tranier训练测试\n",
    "- 内部模型用的也是GPT3ForTextGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "from modelscope.trainers import build_trainer\n",
    "from modelscope.msdatasets import MsDataset\n",
    "from modelscope.utils.hub import read_config\n",
    "from modelscope.metainfo import Metrics, Trainers\n",
    "from datasets import Dataset\n",
    "from modelscope.msdatasets import MsDataset\n",
    "\n",
    "# 模拟训练数据集\n",
    "src_dataset_dict = {\n",
    "    'src_txt': [\n",
    "        '你好啊', '中国是第一名', '我是你的父亲啊'\n",
    "    ]\n",
    "}\n",
    "train_dataset = MsDataset(Dataset.from_dict(src_dataset_dict))\n",
    "\n",
    "print (train_dataset)\n",
    "max_epochs = 1\n",
    "tmp_dir = \"./gpt3_poetry\"\n",
    "\n",
    "num_warmup_steps = 100\n",
    "def noam_lambda(current_step: int):\n",
    "    current_step += 1\n",
    "    return min(current_step ** (-0.5), current_step * num_warmup_steps ** (-1.5))\n",
    "\n",
    "def cfg_modify_fn(cfg):\n",
    "    cfg.train.lr_scheduler = {\n",
    "        \"type\": \"LambdaLR\",\n",
    "        \"lr_lambda\": noam_lambda,\n",
    "        \"options\": {\"by_epoch\": False}\n",
    "    }\n",
    "    cfg.train.optimizer = {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"lr\": 1e-4\n",
    "    }\n",
    "    cfg.train.dataloader = {\"batch_size_per_gpu\": 16, \"workers_per_gpu\": 1}\n",
    "    return cfg\n",
    "\n",
    "kwargs = dict(\n",
    "    model='damo/nlp_gpt3_text-generation_chinese-large',\n",
    "    train_dataset=train_dataset,\n",
    "    max_epochs=max_epochs,\n",
    "    work_dir=tmp_dir,\n",
    "    cfg_modify_fn=cfg_modify_fn)\n",
    "\n",
    "# 构造 trainer 并进行训练\n",
    "trainer = build_trainer(\n",
    "    name=Trainers.nlp_base_trainer, default_args=kwargs)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 这 是 一 个 测 试 文 本 的 网 站 ， 你 可 以 用 它 来 帮 助 自 己 在 你 的 网 站 上 发 布 一 个 文 件 。 你 可 以 通 过 这 种 网 站 ， 来 帮 助 自 己 在 你 的 网 站 上 发 布 一 个 文 件 。 这 是 一 个 很 棒 的 网 站 ， 你 可 以 用 这 个 网 站 ， 来 帮 助 自 己 在 你 的 网 站 上 发 布 一 个 文 件 。 这 是 一 个 很 棒 的 网 站 ， 你 可 以 用 它 来 帮 助 自 己 在 你 的 网 站 上 发'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model = trainer.model.cpu()\n",
    "tokenizer.decode(trainer.model.generate(result)['sequences'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('modelscope')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1bbee1da492aaad4d3f3320eac8e5254725f27b88172569bf99c4c03ddf0c74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
